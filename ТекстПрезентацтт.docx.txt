Регрессионный анализ

Введение: определение задачи и значение

# 1 Слайд - Вступление Регрессионный анализ

# 2 Слайд - Что такое регрессионный анализ

Регрессионный анализ — это статистический метод, позволяющий исследовать связь переменных. Основная цель регрессионного анализа — предсказать значение одной переменной (зависимой переменной, или отклика) на основе одной или нескольких других переменных (независимых переменных, или предикторов). При этом регрессионный анализ помогает выявить, насколько тесно эти переменные связаны и как одна из них влияет на другую.

# 3 слайд - История

Регрессионный анализ – это мощный статистический инструмент, который открывает перед исследователями возможность глубокого понимания взаимосвязей между различными явлениями. Этот метод позволяет:

1. Создавать точные прогнозы будущих значений на основе исторических данных и выявленных закономерностей
2. Количественно оценивать влияние каждого фактора на конечный результат, что особенно ценно для принятия обоснованных решений
3. Разрабатывать сложные математические модели, способные описывать реальные процессы и явления

Увлекательная история развития регрессионного анализа:

XVIII–XIX века - Зарождение метода:
- Первые шаги были сделаны в астрономии и геодезии, где точность измерений имела критическое значение
- Гениальный математик А. М. Лежандр в 1805 году и великий Карл Фридрих Гаусс в 1809 году независимо друг от друга разработали революционный метод наименьших квадратов
- Выдающийся учёный Пьер-Симон Лаплас совершил прорыв, связав этот метод с теорией ошибок, что заложило фундамент современной статистики

Начало XX века - Теоретическое обоснование:
- Русский математик А. А. Марков совершил важный шаг, интегрировав метод наименьших квадратов в математическую статистику
- Активное развитие получила теория оценивания, что позволило делать более точные выводы из данных

Середина XX века - Новые горизонты:
- В 1944 году Джозеф Берксон представил миру логистическую регрессию, открыв новые возможности для анализа бинарных данных
- Появились инновационные методы моделирования вероятностей, расширившие сферу применения регрессионного анализа

Конец XX – начало XXI века - Эра инноваций:
- Развитие регуляризованных методов, включая:
  * Ридж-регрессию (Hoerl & Kennard, 1970) - метод борьбы с мультиколлинеарностью
  * Лассо (Тибширани, 1996) - метод отбора признаков
  * Elastic Net (Zou & Hastie, 2005) - комбинированный подход
- Появление и бурное развитие регрессионных деревьев (CART, random forest)
- Тесная интеграция с современными методами машинного обучения, что открыло новые перспективы в анализе данных

# 4 - Какие методы решрессионного анализа существуют?

Основные методы регрессионного анализа:

1. Линейная регрессия:
   - Простая линейная регрессия (одна независимая переменная)
   - Множественная линейная регрессия (несколько независимых переменных)

2. Нелинейная регрессия:
   - Полиномиальная регрессия
   - Экспоненциальная регрессия
   - Логарифмическая регрессия
   - Степенная регрессия

3. Логистическая регрессия:
   - Бинарная логистическая регрессия
   - Мультиномиальная логистическая регрессия
   - Порядковая логистическая регрессия

4. Регуляризованные методы:
   - Ридж-регрессия (L2-регуляризация)
   - Лассо (L1-регуляризация)
   - Elastic Net (комбинация L1 и L2)

5. Другие методы:
   - Пуассоновская регрессия
   - Регрессия Кокса
   - Регрессионные деревья
   - Метод опорных векторов (SVR)

6 слайд - Иллюстрация линейной регрессии

# Линейная регрессия - визуализация

## Графическое представление
- Точки на графике: исходные данные
- Прямая линия: линия регрессии
- Вертикальные отрезки: отклонения от линии регрессии

## Ключевые элементы
1. Линия регрессии:
   - Проходит через "центр" данных
   - Минимизирует сумму квадратов отклонений
   - Имеет уравнение y = ax + b

2. Отклонения:
   - Показывают разницу между реальными и предсказанными значениями
   - Используются для оценки качества модели
   - В сумме дают минимальное значение

3. Коэффициенты:
   - a (угловой коэффициент): показывает наклон линии
   - b (свободный член): точка пересечения с осью Y

## Интерпретация
- Чем ближе точки к линии, тем лучше модель
- Наклон линии показывает силу и направление связи
- Расстояние от точек до линии характеризует ошибку модели


7 слайд - Пример простой линейной регрессии
# Простая линейная регрессия

## Иллюстрация метода
На рисунке представлены два набора данных с построенными линиями регрессии:
- Верхний график (синие точки): сильная линейная связь (R² ≈ 98,9%)
- Нижний график (красные точки): слабая линейная связь (R² ≈ 57,1%)

## Основные понятия
- Линия регрессии имеет вид: y = ax + b
- Метод наименьших квадратов минимизирует сумму квадратов отклонений
- Коэффициент детерминации (R²) показывает качество модели:
  * R² = 1: идеальное соответствие
  * R² = 0: модель не объясняет данные

# Множественная линейная регрессия

## Определение
Множественная линейная регрессия использует две или более независимых переменных.

## Математическая модель
Y = b₀ + b₁x₁ + b₂x₂ + ... + bₙxₙ

## Особенности
1. Использует гиперплоскость вместо линии регрессии
2. Параметры вычисляются методом наименьших квадратов
3. Позволяет увеличить объяснённую дисперсию

## Преимущества
- Улучшает соответствие модели данным
- Коэффициент детерминации растёт с добавлением переменных

## Проблемы
1. Мультиколлинеарность
2. Выбор оптимальной модели:
   - Информационный критерий Акаике
   - Информационный критерий Байеса
   - Информационный критерий Ханнана-Куина


Множественная линейная регрессия представляет собой статистический метод, который расширяет возможности простой линейной регрессии, позволяя анализировать влияние нескольких независимых переменных на зависимую переменную. 

Практический пример: при прогнозировании стоимости недвижимости модель учитывает комплекс факторов:
- Площадь помещения (x₁)
- Количество комнат (x₂)
- Расстояние до центра (x₃)
- Возраст здания (x₄)
- Этажность (x₅)

Каждый фактор вносит свой вклад в итоговую цену, при этом их влияние может быть различным по силе и направлению. Множественная регрессия позволяет количественно оценить вклад каждого фактора и построить комплексную модель прогнозирования.

Математическая модель:
Y = b₀ + b₁x₁ + b₂x₂ + ... + bₙxₙ
где:
- Y - зависимая переменная (например, цена дома)
- b₀ - свободный член (базовая цена)
- b₁, b₂, ..., bₙ - коэффициенты регрессии, показывающие влияние каждого фактора
- x₁, x₂, ..., xₙ - независимые переменные (факторы)


# 10 слайд - Полиномиальная регрессия

Полиномиальная регрессия применяется для моделирования нелинейных зависимостей между переменными, которые могут быть выражены полиномом степени n. Этот метод особенно эффективен, когда данные демонстрируют криволинейные паттерны, такие как квадратичные (параболические) или кубические зависимости. В отличие от линейной регрессии, полиномиальная модель способна улавливать более сложные нелинейные взаимосвязи, что делает её незаменимым инструментом для анализа данных с криволинейной структурой.

Математическая модель полиномиальной регрессии:
Y = b₀ + b₁x + b₂x² + b₃x³ + ... + bₙxⁿ

где:
- Y - зависимая переменная
- b₀ - свободный член
- b₁, b₂, b₃, ..., bₙ - коэффициенты регрессии
- x - независимая переменная
- n - степень полинома

Пример квадратичной регрессии (n=2):
Y = b₀ + b₁x + b₂x²

Пример кубической регрессии (n=3):
Y = b₀ + b₁x + b₂x² + b₃x³


# 13 слайд - Полиномиальная Регрессия

Полиномиальная регрессия является мощным инструментом для анализа данных, когда характер взаимосвязи между переменными не очевиден или когда требуется учесть влияние нескольких независимых факторов на зависимую переменную.

Однако важно помнить о необходимости критического подхода к интерпретации результатов регрессионного анализа.

Рассмотрим пример: если наблюдается, что у высоких родителей рождаются дети выше среднего роста, а у низких родителей - дети среднего роста, это не означает, что со временем все люди станут одинакового роста. Это явление, известное как регрессия к среднему, демонстрирует важность понимания статистических закономерностей и их правильной интерпретации.


# 14 слайд Логистическая регрессия

Логистическая регрессия - это статистический метод, используемый для анализа данных, где зависимая переменная является бинарной (принимает только два значения, например, "да"/"нет", "успех"/"неудача", "1"/"0"). В отличие от линейной регрессии, логистическая регрессия моделирует вероятность наступления события, преобразуя линейную комбинацию входных переменных через сигмоидную функцию. Это делает её незаменимым инструментом в задачах бинарной классификации, таких как прогнозирование вероятности заболевания, оценки кредитоспособности или определения вероятности покупки товара.

Математическая модель логистической регрессии:

P(Y=1|X) = 1 / (1 + e^(-z))

где:
- P(Y=1|X) - вероятность того, что зависимая переменная Y примет значение 1 при заданных значениях независимых переменных X
- e - основание натурального логарифма (≈ 2.71828)
- z - линейная комбинация независимых переменных: z = b₀ + b₁x₁ + b₂x₂ + ... + bₙxₙ
- b₀, b₁, b₂, ..., bₙ - коэффициенты регрессии
- x₁, x₂, ..., xₙ - независимые переменные

Функция 1/(1 + e^(-z)) называется сигмоидной функцией и преобразует линейную комбинацию z в вероятность от 0 до 1.


# 15 слайд - Картинка

# 16 слайд - Где пригодится?	

Оценки кредитоспособности заемщиков. Например, зависимая переменная будет равняться 1, если клиент расплатится по кредиту и 0, если нет.

Определения вероятности достижения цели при каких-то условиях. Например, будет ли выполнен финансовый план при ведении определенной финансовой политики.

Диагностики финансового состояния компании. Можно оценить влияние таких показателей, как ликвидность, рентабельность, финансовая устойчивость на положение компании.  

Предупреждения состояния банкротства. Метод логистической регрессии позволяет сделать вывод принадлежит ли предприятие к группе банкротов и оценить вероятность возникновения риска банкротства для предприятия.

# 18 слайд - Применение регрессионного анализа

 Экономика и финансы
 Маркетинг и продажи
 Медицина и биология
 Инженерия и производство
 Социология и психология
 Спорт

# 19 слайд - Этапы проведения регрессионного анализа

Постановка задачи и определение цели
Сбор и подготовка данных
Выбор метода регрессии
Построение и обучение модели
Оценка качества модели
Интерпретация результатов.
Прогнозирование и использование модели
Проверка и актуализация модели
Оценка качества модели
Интерпретация результатов.
Прогнозирование и использование модели
Проверка и актуализация модели

## Слайд 23: Пример множественного регрессионного анализа - Постановка задачи

Цель анализа – построить модель множественной линейной регрессии, которая будет прогнозировать цену квартиры в Москве (зависимая переменная, в млн руб.) на основе её характеристик (площадь, этаж, расстояние от центра и т.д.). Такая модель позволяет количественно оценить влияние каждого фактора на стоимость недвижимости. Множественная линейная регрессия «позволяет определить влияние более чем одной независимой переменной на зависимую переменную»
fin-accounting.ru

# Слайд 24 - уравнение

. В нашем примере мы хотим найти уравнение вида

Price_i = β_0 + β_1·Area_i + β_2·Distance_i + β_3·Floor_i + β_4·Age_i + ε_i

где каждое β – коэффициент регрессии, характеризующий влияние соответствующей переменной. Основная задача – определить эти коэффициенты и проверить их статистическую значимость, чтобы понять, какие факторы реально влияют на цену квартиры, а затем использовать модель для прогнозирования.

# Слайд 25 - Описание данных

Для примера возьмём вымышленный, но реалистичный набор данных о квартирах в Москве. Строка данных – одна квартира, столбцы – признаки. Зависимая переменная:
Цена (Price) – цена квартиры в млн руб. (например, от 5 до 30 млн руб.).

Независимые переменные (признаки):
Площадь (Area) – площадь квартиры в квадратных метрах. Типичные значения: 10–100 м².
Количество комнат (Rooms) – число комнат (например, 1–5).
Расстояние до центра (Distance) – расстояние до центра города в км (примерно 1–30 км).
Этаж (Floor) – этаж, на котором находится квартира (например, 1–25).
Возраст здания (Age) – возраст дома в годах (например, 0–100 лет).

# Слайд 26 - таблица

Каждая запись – конкретная квартира, например: площадь 50 м², 3 комнаты, 10 км от центра, этаж 5, возраст 20 лет, цена 15 млн. Приведём фрагмент такого набора данных (пять примеров):

Видно, что с увеличением площади и комнат, как правило, растет цена; чем дальше от центра, обычно тем ниже цена; аналогично старые дома обычно дешевле. На таких данных будем строить модель.

# Предобработка

Перед построением модели проверяем данные: отсутствуют ли пропуски и выбросы. В нашем примере данных несложно предположить, что пропусков нет. Все признаки числовые, дополнительные преобразования (например, создание dummy-переменных для категорий) не требуются. Для регрессии также полезно проверить корреляцию признаков — например, площадь и число комнат могут быть скоррелированы (чем больше площадь, тем больше комнат). Сильная мультиколлинеарность ухудшает оценку коэффициентов, но в учебном примере мы ограничимся простым регрессором. Отдельно можно центрировать или нормировать признаки, но для линейной регрессии это не влияет на точечные оценки коэффициентов – важно только, чтобы не было ярко выраженных аномалий или ошибок измерения.

Для оценки коэффициентов используется метод наименьших квадратов (МНК). После обучения модели на наших данных получили следующие оценки (приведены с точностью до двух знаков):

Это означает, что модель приписала положительный вклад площади и этажа, отрицательный – расстоянию и возрасту. Коэффициент детерминации для этой модели (R²) составляет примерно 0.337, то есть модель объясняет около 33.7% вариации цен в данных. Напомним, что коэффициент детерминации R² показывает «долю дисперсии зависимой переменной, объясненную моделью»
wiki.loginom.ru
. Чем ближе R² к 1, тем лучше модель описывает данные; если R² близок к 0, модель мало объясняет изменений Y.



Интерпретация коэффициентов
Каждый коэффициент β_i в модели отражает изменение ожидаемой цены при единичном изменении соответствующей переменной, если остальные факторы фиксированы (то есть ceteris paribus). Это стандартное свойство линейной регрессии
fin-accounting.ru
. В нашем случае:
β_area = 0.17 (площадь). Значит, при прочих равных, увеличение площади на 1 м² в среднем увеличивает цену на 0.17 млн руб.
β_distance = –0.13 (расстояние). Каждые дополнительные 1 км от центра снижают цену примерно на 0.13 млн руб (меньше спрос в отдалении).
β_floor = 0.09 (этаж). Рост на 1 этаж даёт прибавку ~0.09 млн руб. Возможно, более высокие этажи ценятся из-за видов или отсутствия соседей снизу.
β_age = –0.04 (возраст). Каждые +1 год возраста дома уменьшают цену на 0.04 млн руб (более старые дома дешевле, другие факторы равны).
Например, β_area=0.17 означает: «если все остальные характеристики квартиры (расстояние, этаж, возраст) одинаковы, то квартира площадью 60 м² будет в среднем дороже квартиры 59 м² на 0.17 млн руб»
fin-accounting.ru
. Аналогично, отрицательные β_distance и β_age указывают на обратную зависимость: увеличение расстояния или возраста ведёт к снижению цены, при прочих равных.


Статистическая значимость (p-значения)
При построении регрессии важно не только получить оценки β, но и понять, какие из них статистически значимы. Для каждого коэффициента проверяется нулевая гипотеза H₀: «коэффициент равен нулю» (то есть признак не влияет на цену). Статистика (t-статистика) и соответствующее ей p-значение показывают, насколько мы можем отвергнуть H₀. Небольшое p-значение (обычно <0.05) говорит, что H₀ маловероятна, то есть признак значим. Как правило: «чем меньше p-значение, тем больше доказательств против нулевой гипотезы»
fin-accounting.ru
. В нашей модели p-значения получились примерно такие (при α=0.05):
Площадь: p < 0.001 – коэффициент значим на очень высоком уровне.
Расстояние: p ≈ 0.063 – не значительно при α=0.05 (около границы значимости).
Этаж: p ≈ 0.294 – не значимо (нет статистически надежного эффекта).
Возраст: p ≈ 0.062 – не значимо на уровне 0.05 (слабая значимость).
То есть в модели с уровнем значимости 5% только площадь можно считать значимым предиктором цены. Коротко говоря, «низкое p-значение означает сильное доказательство против нулевой гипотезы»
fin-accounting.ru
, поэтому влияет ли признак на цену. Для остальных переменных p-значение выше 0.05, поэтому их влияние по статистике не отличается от нуля при типичном уровне значимости.


Статистическая значимость (p-значения)
При построении регрессии важно не только получить оценки β, но и понять, какие из них статистически значимы. Для каждого коэффициента проверяется нулевая гипотеза H₀: «коэффициент равен нулю» (то есть признак не влияет на цену). Статистика (t-статистика) и соответствующее ей p-значение показывают, насколько мы можем отвергнуть H₀. Небольшое p-значение (обычно <0.05) говорит, что H₀ маловероятна, то есть признак значим. Как правило: «чем меньше p-значение, тем больше доказательств против нулевой гипотезы»
fin-accounting.ru
. В нашей модели p-значения получились примерно такие (при α=0.05):
Площадь: p < 0.001 – коэффициент значим на очень высоком уровне.
Расстояние: p ≈ 0.063 – не значительно при α=0.05 (около границы значимости).
Этаж: p ≈ 0.294 – не значимо (нет статистически надежного эффекта).
Возраст: p ≈ 0.062 – не значимо на уровне 0.05 (слабая значимость).
То есть в модели с уровнем значимости 5% только площадь можно считать значимым предиктором цены. Коротко говоря, «низкое p-значение означает сильное доказательство против нулевой гипотезы»
fin-accounting.ru
, поэтому влияет ли признак на цену. Для остальных переменных p-значение выше 0.05, поэтому их влияние по статистике не отличается от нуля при типичном уровне значимости.


Рис.1. Схема зависимости между реальными (ось Y) и независимыми (ось X) переменными с прямой линейной регрессии. На диаграмме (scatter-плоте) показан пример расположения точек (значения из данных) и построенная по ним линия линейной регрессии. Если бы мы строили график фактические vs предсказанные, то при идеальном прогнозе все точки легли бы вдоль диагонали y=x. В реальных данных точки рассеиваются вокруг этой линии: чем ближе точки к диагонали, тем точнее модель предсказывает значения. На рисунке видно, что модель дает адекватную аппроксимацию: большинство точек лежит относительно близко к линии (зелёная прямая).


График остатков

График остатков (например, остатки 
𝑒
𝑖
=
𝑦
𝑖
−
𝑦
^
𝑖
e 
i
​
 =y 
i
​
 − 
y
^
​
  
i
​
  против предсказанных или по порядку наблюдений) важен для диагностики. Идеально – точки остатков должны быть случайно разбросаны вокруг нуля без явной структуры. Если на графике остатков виден тренд или «всё вдвое», значит модель не учла какую-то зависимость. Также строят гистограмму или Q-Q график остатков, чтобы проверить нормальность ошибок. В хорошем случае гистограмма будет примерно симметричной, близкой к нормальному распределению, а точечный график не демонстрирует паттернов. В нашей учебной модели остатков немного – их распределение можно считать приемлемым (примерно случайный шум с нормальным распределением).


Важность переменных

Для линейной регрессии «важность» признака часто оценивается по абсолютному значению его коэффициента (или стандартизированного коэффициента). Чем больше |β_j|, тем сильнее влияние признака на цену. В нашей модели наибольший по модулю коэффициент имеет площадь (β≈0.17), затем расстояние (|β|≈0.13), дальше этаж (0.09) и возраст (0.04). Это согласуется с выводом о значимости: наибольшая важность у площади. Визуально это можно отобразить столбчатой диаграммой: например, столбцы высотой |β| для каждого фактора. Таким образом, наибольший вклад в прогноз цены (самый высокий столбец) у площади, а у возраста – наименьший. Такой «график важности» помогает понять, какие факторы модель учитывает сильнее.

Пример прогноза

Предположим, нужно спрогнозировать цену для новой квартиры, заданной признаками. Воспользуемся полученной моделью. Например, возьмём: площадь = 60 м², расстояние до центра = 10 км, этаж = 5, возраст дома = 20 лет. Подставим значения в уравнение модели:
𝑃
𝑟
𝑖
𝑐
𝑒
^
=
9.05
+
0.17
×
60
−
0.13
×
10
+
0.09
×
5
−
0.04
×
20.
Price
^
 =9.05+0.17×60−0.13×10+0.09×5−0.04×20.
Посчитаем: 
9.05
+
10.2
−
1.3
+
0.45
−
0.8
=
17.6
9.05+10.2−1.3+0.45−0.8=17.6. Получаем примерно 17.6 млн руб. То есть модель предсказывает, что такая квартира будет стоить около 17.6 млн руб. Аналогично можно подставлять любые другие значения признаков для прогноза.

Выводы

В нашем примере модель множественной линейной регрессии показала, что площадь квартиры является наиболее значимым фактором при прогнозировании цены (при прочих равных большой β и низкое p-value). Чем больше площадь, тем выше цена (β>0). Расстояние до центра оказывает умеренно значимое отрицательное влияние (β≈–0.13, p≈0.06 – близко к порогу значимости), т.е. более удалённые квартиры дешевле. Этаж и возраст в данной модели статистически незначимы (p > 0.05), что означает, что для нашего примера их эффект может быть поглощён шумом или другими факторами. Коэффициент детерминации модели R²≈0.34 указывает, что учтённые переменные объясняют примерно 34% разброса цен
wiki.loginom.ru
. Это говорит о том, что модель описывает данные лишь частично – кроме указанных факторов на цену, вероятно, влияют и другие характеристики (район города, близость метро, состояние квартиры и т.д.), которые мы не учли.

Практическое применение такой модели: с её помощью можно быстро получать грубую оценку рыночной стоимости квартиры по известным характеристикам и выявлять основные драйверы цены (например, площадь). Полученные коэффициенты можно интерпретировать и использовать в аналитике недвижимости. Однако перед практическим прогнозированием важно убедиться, что допущения линейной регрессии выполняются (линейность взаимосвязи, нормальность распределения ошибок, однородность дисперсии остатков и пр.)
fin-accounting.ru
. Если эти допущения сильно нарушаются, простая модель может быть неточной. В целом, множественная регрессия оказалась полезным инструментом: она выявила главные зависимости и позволила сделать осмысленные выводы о ценообразовании квартир.